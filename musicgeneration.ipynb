{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f31aa606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas matplotlib scipy seaborn tensorflow music21 keras numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4eaa844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a94138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to read MIDI files\n",
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73376310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: data/1.mid\n",
      "Loading Music File: data/2.mid\n",
      "Loading Music File: data/3.mid\n",
      "Loading Music File: data/4.mid\n",
      "Loading Music File: data/5.mid\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "#for listing down the file names\n",
    "import os\n",
    "\n",
    "#Array Processing\n",
    "import numpy as np\n",
    "\n",
    "#specify the path\n",
    "path='data/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files], dtype=object)\n",
    "\n",
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d64fe95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([122.,  20.,  11.,   9.,   0.,   3.,   1.,   1.,   0.,   1.]),\n",
       " array([  1. ,  18.1,  35.2,  52.3,  69.4,  86.5, 103.6, 120.7, 137.8,\n",
       "        154.9, 172. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 428,
       "width": 445
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing library\n",
    "from collections import Counter\n",
    "\n",
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "#library for visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3152100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=20]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe3c2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3af535b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14432631",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a01c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing input sequences\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19f98ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0474a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a5fb216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm():\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(128,return_sequences=True))\n",
    "  model.add(LSTM(128))\n",
    "  model.add(Dense(256))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dense(n_vocab))\n",
    "  model.add(Activation('softmax'))\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "510ee986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 100)           4500      \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 32, 64)            19264     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 16, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 16, 128)           24704     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 8, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 8, 256)            98560     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 256)            0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 4, 256)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 45)                11565     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224,385\n",
      "Trainable params: 224,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "    \n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aed8a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f9ddc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 3.7538\n",
      "Epoch 1: val_loss improved from inf to 3.65195, saving model to best_model.h5\n",
      "13/13 [==============================] - 3s 118ms/step - loss: 3.7482 - val_loss: 3.6520\n",
      "Epoch 2/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 3.6099\n",
      "Epoch 2: val_loss improved from 3.65195 to 3.58974, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 3.6080 - val_loss: 3.5897\n",
      "Epoch 3/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 3.5368\n",
      "Epoch 3: val_loss improved from 3.58974 to 3.53650, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 3.5408 - val_loss: 3.5365\n",
      "Epoch 4/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 3.4556\n",
      "Epoch 4: val_loss improved from 3.53650 to 3.43553, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 3.4555 - val_loss: 3.4355\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 3.3108\n",
      "Epoch 5: val_loss improved from 3.43553 to 3.31640, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 3.3125 - val_loss: 3.3164\n",
      "Epoch 6/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 3.1501\n",
      "Epoch 6: val_loss improved from 3.31640 to 3.15188, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 3.1414 - val_loss: 3.1519\n",
      "Epoch 7/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 2.9793\n",
      "Epoch 7: val_loss improved from 3.15188 to 3.02652, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 2.9836 - val_loss: 3.0265\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 2.7950\n",
      "Epoch 8: val_loss improved from 3.02652 to 2.87216, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 2.7950 - val_loss: 2.8722\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 2.6201\n",
      "Epoch 9: val_loss improved from 2.87216 to 2.78135, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 2.6201 - val_loss: 2.7814\n",
      "Epoch 10/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 2.5176\n",
      "Epoch 10: val_loss improved from 2.78135 to 2.71436, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 2.5055 - val_loss: 2.7144\n",
      "Epoch 11/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 2.3936\n",
      "Epoch 11: val_loss improved from 2.71436 to 2.60972, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 2.4011 - val_loss: 2.6097\n",
      "Epoch 12/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 2.2636\n",
      "Epoch 12: val_loss improved from 2.60972 to 2.53014, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 2.2598 - val_loss: 2.5301\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 2.1691\n",
      "Epoch 13: val_loss improved from 2.53014 to 2.44950, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 2.1594 - val_loss: 2.4495\n",
      "Epoch 14/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 2.0346\n",
      "Epoch 14: val_loss improved from 2.44950 to 2.36786, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 2.0440 - val_loss: 2.3679\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.9878\n",
      "Epoch 15: val_loss improved from 2.36786 to 2.33637, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 1.9878 - val_loss: 2.3364\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.9398\n",
      "Epoch 16: val_loss improved from 2.33637 to 2.27779, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 101ms/step - loss: 1.9398 - val_loss: 2.2778\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8482\n",
      "Epoch 17: val_loss improved from 2.27779 to 2.22847, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 1.8482 - val_loss: 2.2285\n",
      "Epoch 18/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.7845\n",
      "Epoch 18: val_loss improved from 2.22847 to 2.12062, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 1.7921 - val_loss: 2.1206\n",
      "Epoch 19/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.7045\n",
      "Epoch 19: val_loss improved from 2.12062 to 2.07617, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 1.7008 - val_loss: 2.0762\n",
      "Epoch 20/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.6067\n",
      "Epoch 20: val_loss improved from 2.07617 to 2.02056, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 1.6173 - val_loss: 2.0206\n",
      "Epoch 21/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.5773\n",
      "Epoch 21: val_loss improved from 2.02056 to 1.96283, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 1.5837 - val_loss: 1.9628\n",
      "Epoch 22/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.5044\n",
      "Epoch 22: val_loss improved from 1.96283 to 1.95969, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 1.5069 - val_loss: 1.9597\n",
      "Epoch 23/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.4355\n",
      "Epoch 23: val_loss improved from 1.95969 to 1.88115, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 1.4426 - val_loss: 1.8811\n",
      "Epoch 24/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.3788\n",
      "Epoch 24: val_loss improved from 1.88115 to 1.85905, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 1.3799 - val_loss: 1.8590\n",
      "Epoch 25/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.3246\n",
      "Epoch 25: val_loss improved from 1.85905 to 1.81650, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 1.3325 - val_loss: 1.8165\n",
      "Epoch 26/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.2819\n",
      "Epoch 26: val_loss improved from 1.81650 to 1.73318, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 1.2917 - val_loss: 1.7332\n",
      "Epoch 27/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 1.2279\n",
      "Epoch 27: val_loss improved from 1.73318 to 1.72173, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 1.2325 - val_loss: 1.7217\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2094\n",
      "Epoch 28: val_loss improved from 1.72173 to 1.69885, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 1.2094 - val_loss: 1.6989\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1754\n",
      "Epoch 29: val_loss improved from 1.69885 to 1.64359, saving model to best_model.h5\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 1.1754 - val_loss: 1.6436\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1124\n",
      "Epoch 30: val_loss improved from 1.64359 to 1.58297, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 116ms/step - loss: 1.1124 - val_loss: 1.5830\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0724\n",
      "Epoch 31: val_loss did not improve from 1.58297\n",
      "13/13 [==============================] - 2s 146ms/step - loss: 1.0724 - val_loss: 1.5892\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0088\n",
      "Epoch 32: val_loss improved from 1.58297 to 1.54120, saving model to best_model.h5\n",
      "13/13 [==============================] - 2s 170ms/step - loss: 1.0088 - val_loss: 1.5412\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9611\n",
      "Epoch 33: val_loss did not improve from 1.54120\n",
      "13/13 [==============================] - 2s 151ms/step - loss: 0.9611 - val_loss: 1.5624\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9656\n",
      "Epoch 34: val_loss improved from 1.54120 to 1.52897, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 2s 185ms/step - loss: 0.9656 - val_loss: 1.5290\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8887\n",
      "Epoch 35: val_loss improved from 1.52897 to 1.47095, saving model to best_model.h5\n",
      "13/13 [==============================] - 2s 182ms/step - loss: 0.8887 - val_loss: 1.4710\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8605\n",
      "Epoch 36: val_loss did not improve from 1.47095\n",
      "13/13 [==============================] - 2s 175ms/step - loss: 0.8605 - val_loss: 1.4865\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8381\n",
      "Epoch 37: val_loss improved from 1.47095 to 1.44679, saving model to best_model.h5\n",
      "13/13 [==============================] - 2s 153ms/step - loss: 0.8381 - val_loss: 1.4468\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8096\n",
      "Epoch 38: val_loss did not improve from 1.44679\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.8096 - val_loss: 1.4697\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8017\n",
      "Epoch 39: val_loss improved from 1.44679 to 1.43537, saving model to best_model.h5\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.8017 - val_loss: 1.4354\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7794\n",
      "Epoch 40: val_loss did not improve from 1.43537\n",
      "13/13 [==============================] - 2s 130ms/step - loss: 0.7794 - val_loss: 1.4443\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7605\n",
      "Epoch 41: val_loss did not improve from 1.43537\n",
      "13/13 [==============================] - 2s 157ms/step - loss: 0.7605 - val_loss: 1.4365\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7462\n",
      "Epoch 42: val_loss improved from 1.43537 to 1.43155, saving model to best_model.h5\n",
      "13/13 [==============================] - 2s 147ms/step - loss: 0.7462 - val_loss: 1.4316\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6969\n",
      "Epoch 43: val_loss improved from 1.43155 to 1.41248, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 101ms/step - loss: 0.6969 - val_loss: 1.4125\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6743\n",
      "Epoch 44: val_loss improved from 1.41248 to 1.40050, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 111ms/step - loss: 0.6743 - val_loss: 1.4005\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6658\n",
      "Epoch 45: val_loss improved from 1.40050 to 1.39053, saving model to best_model.h5\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 0.6658 - val_loss: 1.3905\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6300\n",
      "Epoch 46: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 0.6300 - val_loss: 1.4332\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6101\n",
      "Epoch 47: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 0.6101 - val_loss: 1.4121\n",
      "Epoch 48/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5703\n",
      "Epoch 48: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.5802 - val_loss: 1.4566\n",
      "Epoch 49/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5809\n",
      "Epoch 49: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.5903 - val_loss: 1.4495\n",
      "Epoch 50/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5405\n",
      "Epoch 50: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.5445 - val_loss: 1.4037\n",
      "Epoch 51/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5836\n",
      "Epoch 51: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.5862 - val_loss: 1.4154\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5423\n",
      "Epoch 52: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 0.5423 - val_loss: 1.4402\n",
      "Epoch 53/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5330\n",
      "Epoch 53: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.5416 - val_loss: 1.4383\n",
      "Epoch 54/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4960\n",
      "Epoch 54: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.5170 - val_loss: 1.4466\n",
      "Epoch 55/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5068\n",
      "Epoch 55: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.5157 - val_loss: 1.4218\n",
      "Epoch 56/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5196\n",
      "Epoch 56: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.5192 - val_loss: 1.4600\n",
      "Epoch 57/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5097\n",
      "Epoch 57: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.5029 - val_loss: 1.4392\n",
      "Epoch 58/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4825\n",
      "Epoch 58: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.4907 - val_loss: 1.4693\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5050\n",
      "Epoch 59: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.5050 - val_loss: 1.4294\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4871\n",
      "Epoch 60: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.4871 - val_loss: 1.4807\n",
      "Epoch 61/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4493\n",
      "Epoch 61: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.4512 - val_loss: 1.4571\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4154\n",
      "Epoch 62: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 0.4154 - val_loss: 1.5155\n",
      "Epoch 63/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4283\n",
      "Epoch 63: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 0.4234 - val_loss: 1.5083\n",
      "Epoch 64/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4418\n",
      "Epoch 64: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.4332 - val_loss: 1.4855\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4177\n",
      "Epoch 65: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.4177 - val_loss: 1.4770\n",
      "Epoch 66/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4229\n",
      "Epoch 66: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.4279 - val_loss: 1.4948\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3804\n",
      "Epoch 67: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 0.3804 - val_loss: 1.5133\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4119\n",
      "Epoch 68: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 0.4119 - val_loss: 1.5374\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3911\n",
      "Epoch 69: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 114ms/step - loss: 0.3911 - val_loss: 1.5201\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3939\n",
      "Epoch 70: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 112ms/step - loss: 0.3939 - val_loss: 1.5009\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3473\n",
      "Epoch 71: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 0.3473 - val_loss: 1.5528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3433\n",
      "Epoch 72: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.3433 - val_loss: 1.5478\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3531\n",
      "Epoch 73: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 0.3531 - val_loss: 1.5533\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3421\n",
      "Epoch 74: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 0.3421 - val_loss: 1.5683\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3050\n",
      "Epoch 75: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 0.3050 - val_loss: 1.6099\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3080\n",
      "Epoch 76: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 0.3080 - val_loss: 1.6318\n",
      "Epoch 77/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3084\n",
      "Epoch 77: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.3068 - val_loss: 1.6066\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2940\n",
      "Epoch 78: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 112ms/step - loss: 0.2940 - val_loss: 1.6228\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2920\n",
      "Epoch 79: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.2920 - val_loss: 1.6695\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3089\n",
      "Epoch 80: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.3089 - val_loss: 1.6563\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3075\n",
      "Epoch 81: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 0.3075 - val_loss: 1.6199\n",
      "Epoch 82/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.2917\n",
      "Epoch 82: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.2940 - val_loss: 1.6492\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2840\n",
      "Epoch 83: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.2840 - val_loss: 1.6746\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2850\n",
      "Epoch 84: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.2850 - val_loss: 1.6943\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2888\n",
      "Epoch 85: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.2888 - val_loss: 1.7056\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3007\n",
      "Epoch 86: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.3007 - val_loss: 1.7055\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2751\n",
      "Epoch 87: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.2751 - val_loss: 1.6637\n",
      "Epoch 88/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.2815\n",
      "Epoch 88: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.2856 - val_loss: 1.6806\n",
      "Epoch 89/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.2870\n",
      "Epoch 89: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.2839 - val_loss: 1.7298\n",
      "Epoch 90/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.2539\n",
      "Epoch 90: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 0.2602 - val_loss: 1.6860\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2501\n",
      "Epoch 91: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.2501 - val_loss: 1.6805\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2409\n",
      "Epoch 92: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.2409 - val_loss: 1.6914\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2484\n",
      "Epoch 93: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 106ms/step - loss: 0.2484 - val_loss: 1.7194\n",
      "Epoch 94/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.2299\n",
      "Epoch 94: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 109ms/step - loss: 0.2364 - val_loss: 1.6860\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2265\n",
      "Epoch 95: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.2265 - val_loss: 1.7675\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2232\n",
      "Epoch 96: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.2232 - val_loss: 1.7801\n",
      "Epoch 97/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.2245\n",
      "Epoch 97: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.2295 - val_loss: 1.8160\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2251\n",
      "Epoch 98: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.2251 - val_loss: 1.8040\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2613\n",
      "Epoch 99: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.2613 - val_loss: 1.7942\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2612\n",
      "Epoch 100: val_loss did not improve from 1.39053\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.2612 - val_loss: 1.7675\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=100, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ebaba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best model\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4e4a525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 308ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[15, 38, 23, 38, 15, 3, 15, 3, 15, 3, 3, 3, 3, 3, 15, 23, 23, 23, 23, 15]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "\n",
    "random_music = x_val[ind]\n",
    "\n",
    "predictions=[]\n",
    "for i in range(20):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7412feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb122128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Tambourine()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Tambourine()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72a897cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0098c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98648f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "96e7530fba79d702a10a5c151c62f6260947651decaec60a6f742e427fa902a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
